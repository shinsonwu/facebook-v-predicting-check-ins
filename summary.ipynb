{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Checkins.\n",
    "\n",
    "\n",
    "Here I will summarise my best submission(0.58685(~116pub)-0.58637(~130pri) based on xgboost), but before that I will try to explain my various approaches to the problem and my learnings during this competition.\n",
    "\n",
    "Here is my code realated to all my approaches. https://github.com/syllogismos/facebook-v-predicting-check-ins\n",
    "\n",
    "This is my first proper shot at a kaggle competition I participated in. I participated in few earlier some 2/3 years back.. but didnt do much, just put the data in some algorithm without really tuning the parameters and etc..\n",
    "\n",
    "I learned a lot from the forums and how to approach the problems. Some validated my approaches and some taught me about how different classifiers work and how to tune them.\n",
    "\n",
    "### I liked two posts specifically.\n",
    "    1. The post about how just submitting the top 3 place ids in a grid cell. This is a really nice intuition. My first intuition is to calculate the median x,y of all the place ids and for each checkin event from test set submit the closest. The top3 approach had a better score than mine. And also made me realize the importance of starting simple and then iterating on our approaches. I also participated one of the earlier facebook competitions facebook link prediction. Given the friend network you have to predict the new friend connections that are most likely to happen. which is called link prediction. Instead of starting simple I started implementing a paper. I understood the math in it and all that, but in the end I wasn't even able to generate a submission using my implementation.  I wrote about the implementation here http://syllogismos.github.io/blog/2014/08/10/facebook-link-prediction/\n",
    "\n",
    "    2. How the time features like hour of the day and etc are converted into periodical features using sin and cos. This didn't really help me much. But made me realize how to go about feature engineering.. for future kaggle competitions :D\n",
    "\n",
    "## My approaches:\n",
    "    1. Initially I divided the training data based on a random split. People in forums suggestd splitting it based on some time. All events after a point of time are cv and before are train. \n",
    "    2. Find the median/mean/mode of all place ids. and then for each checkin event from the test set, find the closest three places. I tried both eucledian and manhattan distances.\n",
    "    3. I also implimented the top 3 places in a each grid cell for all the points falling in the grid.\n",
    "    4. Next I divided the world into mxn square grids with some buffer on both sides and only trained on placeids that came atleast some n number of times. Most of the people in the forums were also doing the same which validated what I wanted to do also. So I made a general frame work where I can put in any classifier in a single grid. I also parallalized how the training happens. Say the world is divided into mxn grids. I would be processing each row parallally. I also used the multiple threads the classifier itself provides. For example, in case of xgboost, I would have nthreads = 4, and would process 8 rows at a time in c4.8xlarge machine. My best submission took around 3.5 hours to train and generate the submission which had 40 rows and 200 columns.\n",
    "\n",
    "    Using this framework I tried, KNeighbours, GaussianNB, and xgboost. I wasn't doing really well on the leader board, was around 600 or something with 10 days to go. At the least I wanted to beat the best open script submission out there. I gave up all hope and then suddenly when I reduced the grid size in my xgboost implementation I fell in top 100 with 6 days to go. Which felt really good.  I tried various ways of improving my lb score. I had three things in mind. \n",
    "        * Instead of using the same params for all the grids, why not do a grid cv search in each cell. Say for my best performing grid till then. There are 40*200 grids, That is 8000 diff grid cv searches... Instead of doing a grid search for all grids, I did a grid search for m, n grid.. and used those params for all the grids that fall into a bigger grid. I implemented this and didn't really find much improvement. And the gridcv search took almost a day to finish as far as I can remember. Not only did it take so much time to implement and finally be able to use those params. It didn't even improve my score. Which was a bummer.\n",
    "        * The other thing I have in my mind was to use the top 10 predictions I did on the training/test data to with one run and some how extract even more features based on the top 10 predicted place ids. All I could think of was to make use of the cardinality of the top 10 predicted place ids, median of the accuracy of those ids, median of x, y.. distance of the check in event from the top 10 places.\n",
    "        Ex: for event x, y, a, t my predictions are p1, p2, p3, p4.. p10 using a classifie clf1\n",
    "        I would generate more features from each of these prdiction p1.. p10\n",
    "        like median of the accuracy of all the p1 events from train data. median position of p1, no of occurances of p1 in training data.. distance of the check in event from median p1. for all the top 10 predictions..\n",
    "        Initially I wanted to try neural nets or some other classification algorithm in my grid framework. But instead I convinced myself, more feature engineering is better than trying a diff model and went ahead with implementing this instead. So now instead of just x, y, a, and time features, I also have all the place id characterestics to predict. After spending considerable amount of time on this. It brought some interesting problems my way also when it came to impelemntation. The training data suddely is around 30 GB now. It did well on my cross validation set but it also didnt improve my lb score. I think because of the total no of features I have now, it is over fitting.. Bummerrr..\n",
    "        * One more thing I had in my mind is that.. out of all the test events.. If I were to somehow be able to identify all the checkin events where the predictions with the second place as the right palce id. As in if my earlier classifier predicted p1, p2, p3 s the predictions.. I have the top n prediction data for the training and cv data also.. say if p2 is sort of the place that is more likely to not be first place.. and if I were to successfully identify that, my score would increase.. I did a validation of this.. So for my cv, about 59% of my predictions are where the p1 the first prediction is the right check in place. and 13% p2, and so on.. And if I were to identify those 13% checkin events successfully.. My score would increase.. But this also didn't work.\n",
    "        \n",
    "        After trying and failing to come up with more features, I'm soo interested in how people who are top in lb got there. Now that I think about it, instead of trying all these hypothesis on getting more features and feature engineering, I should have instead tried a different algorithm :D\n",
    "        In the end I was only able to improve my score slightly. By trying different size grids. and different params and diff thresholds of cardinality of a place.\n",
    "        \n",
    "        OK enough of my essays.. lets get to the actual implementaion and code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# here is the repo, https://github.com/syllogismos/facebook-v-predicting-check-ins\n",
    "# cd facebook-v-predicting-check-ins\n",
    "# have train.csv, cv.csv, test.csv in this folder\n",
    "# jupyter notebook and open this notebook\n",
    "cd scripts/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# scripts/grid_generation.py\n",
    "from grid_generation import Grid\n",
    "\n",
    "# grid defn: x, y, xd, yd\n",
    "# and assume 2*xd < x, 2*yd < y.. don't know if this is necessary.. but for now\n",
    "# im lazy to evaluate\n",
    "# x is the length wise and y is the height wise grid length. xd and yd are the\n",
    "# buffers added on both sides of x and y.\n",
    "# \n",
    "#            ....xxxxxxxxxxxxxxxxxxxx....\n",
    "#            .                          .\n",
    "#            y   xxxxxxxxxxxxxxxxxxxx   y\n",
    "#            y   y                  y   y\n",
    "#         y  y   y                  y   y\n",
    "#            y   xxxxxxxxxxxxxxxxxxxx   y\n",
    "#        yd  .                         .\n",
    "#            ....xxxxxxxxxxxxxxxxxxxx....\n",
    "#             xd          x\n",
    "# \n",
    "# every individual world has all the training data inside the outer rectangle that includes\n",
    "# the buffer.\n",
    "# when a test data point is considered, you select the world in which the test point falls in\n",
    "# the inner rectangle\n",
    "\n",
    "g = Grid(250, 50, 20, 5, pref = 'grid', files_flag = True, train_file = '../train.csv')\n",
    "# 250, 50, 20, 5 are all in meters.\n",
    "# it means a single grid cell is 250 mts wide, 50 mts height, 20 mts buffer along x axis and 5 mts buffer on y axis\n",
    "# so m = 10000/250 and n = 10000/50 a 40X200\n",
    "\n",
    "g.generateCardinalityMatrix()\n",
    "# this will generate all train data into respective grid files, and also calculates the cardinality matrix of all\n",
    "# place ids in respective grid. if files_flag above is True, it will generate 40*200 files. You might have to increase \n",
    "# your ulimit settings of os. don't recommend in local machine but in aws pls go wild\n",
    "# for bigger grid cells you wont have a problem even in local machine.\n",
    "# this also generates a cardinality matrix where g.M[i][j] will have a dictionary of all the place ids and its count \n",
    "# in grid i, j\n",
    "# I wanted to do this computation only once per grid defn, so this is why i did it. I also pickle the cardinality matrix\n",
    "# and save it along with all the individual grid train files in the folder ../grid_250_50_20_5\n",
    "# if you are running generateCardinalityMatrix for the first time, it will create all the files necessary during its first\n",
    "# run only, and keep using those files in later runs as long as you didn't chage the grid defn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# scripts xgboost_time_features.py\n",
    "# you can do a sample xgboost run like this\n",
    "import xgboost_time_features as xgbm\n",
    "\n",
    "\n",
    "xgbm1 = xgbm.XGB_Model(grid = g, threshold = 7, cross_validation_file='../cv.csv')\n",
    "# grid is the above grid object, threshold is the threshold of the cardinality of a place for it to be considered\n",
    "# while training, and cross validation file\n",
    "\n",
    "params1 = {\n",
    "        'objective': 'multi:softprob',\n",
    "        'eta': 0.1,\n",
    "        'max_depth': 5,\n",
    "        'min_child_weight': 4,\n",
    "        'gamma': 0.3,\n",
    "        'subsample': 0.9,\n",
    "        'colsample_bytree': 0.7,\n",
    "        'colsample_bylevel': 0.9,\n",
    "        'scale_pos_weight': 1,\n",
    "        'nthread': 4,\n",
    "        'silent': 1,\n",
    "        'max_delta_step': 6\n",
    "}\n",
    "xgbm1.ini_params(params1)\n",
    "# define the parameters of the xgboost and initialize them. i didn't fix the ini type..\n",
    "name1 = 'grid_200_20_30_10_params_01_5_4_03_09_07_09_6_th7.csv'\n",
    "xgbm1.train_and_predict_parallel(name1, upload_to_s3 = True)\n",
    "# train and predict each grid individually and generate submission file with the name given,\n",
    "# if upload_to_s3 is True, it zips the file and uploads the file to s3. for it to actually work, you have to update\n",
    "# aws secret keys and access key in aws_config.py to yours.\n",
    "# When i started playing with xgboost, I came to a conclusion that I need to do my runs on ec2 instead of my local machine\n",
    "# as it is not very powerful. The above script will start training the data at 8 rows at a time, and in each row each grid\n",
    "# cell will be trained serially with nthread = 4, so if you launch a c4.8xlarge machine all cpus will be used, with sane \n",
    "# load avgs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Above code should work and should be able to generate a submission\n",
    "# Here I will go into the details of the whole implementation. you can find the actual working\n",
    "# code in the files mentioned above in the first lines..\n",
    "# above two code blocks are implemented this way.\n",
    "\n",
    "import time\n",
    "import xgboost as xgb\n",
    "from grid_generation import Grid, generate_grid_wise_cardinality_and_training_files\n",
    "from grid_generation import get_top_3_places_of_dict, get_grids\n",
    "from helpers import days, hours, quarter_days\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from helpers import apk, zip_file_and_upload_to_s3\n",
    "\n",
    "from base_scikit_learn_model import SklearnModel, get_grids_of_a_point\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import pdb\n",
    "\n",
    "from multiprocessing import Pool\n",
    "import logging\n",
    "logging.basicConfig(filename='xgb_time_feature_runs.log', level=logging.DEBUG)\n",
    "\n",
    "def map3eval(preds, dtrain):\n",
    "    \"\"\"\n",
    "    custom evaluation function to be passed along with xgboost\n",
    "    \"\"\"\n",
    "    actual = dtrain.get_label()\n",
    "    predicted = preds.argsort(axis=1)[:,-np.arange(1,4)]\n",
    "    metric = 0.\n",
    "    for i in range(3):\n",
    "        metric += np.sum(actual==predicted[:,i])/(i+1)\n",
    "    metric /= actual.shape[0]\n",
    "    return 'MAP@3', metric\n",
    "\n",
    "class StateLoader(object):\n",
    "    \"\"\"\n",
    "    class that helps in parallelising the code,\n",
    "    this is because of python 2.7 limitation when it comes to\n",
    "    using Pool.map\n",
    "    \n",
    "    using this we can just do something like \n",
    "    p = Pool(8)\n",
    "    results = p.map(StateLoader(state), [1..m])\n",
    "    p.close()\n",
    "    p.join()\n",
    "    \"\"\"\n",
    "    def __init__(self, state):\n",
    "        self.state = state\n",
    "    def __call__(self, row):\n",
    "        return train_row(row, self.state)\n",
    "\"\"\"\n",
    "State:\n",
    "    grid\n",
    "    cv_grid\n",
    "    test_grid\n",
    "state = {\n",
    "    'grid': Grid, the grid object\n",
    "    'cv_grid': cv data converted to the grid specs, state['cv_data'][m][n] has the cv data that belong m,n grid\n",
    "    'test_grid': same with test\n",
    "    'threshold': 5\n",
    "    'params_dict': xgb params for each grid\n",
    "    'folder': {for giggles, i mean to save files} ignore this\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def train_row(i, state):\n",
    "    \"\"\"\n",
    "    given the row number and the state, that has the grid object, cv/test data grid wise\n",
    "    it will return test predictions, cv predictions and etc.\n",
    "    \"\"\"\n",
    "    print \"processing row %s\" %(i)\n",
    "    init_row_time = time.time()\n",
    "    test_preds = []\n",
    "    cv_preds = []\n",
    "    train_preds = []\n",
    "    for n in range(state['grid'].max_n + 1):\n",
    "    # for n in range(1):\n",
    "        if n % 10 == 0:\n",
    "            print \"processing column %s of row %s\" %(n, i)\n",
    "        clf, x_transformer, y_transformer, top_t = train_single_grid_cell(i, n, state)\n",
    "        if top_t_train_places != None:\n",
    "            train_preds.append(top_t_train_places)\n",
    "        if len(state['test_grid'][i][n]) > 0:\n",
    "            test_preds.append(predict_single_grid_cell(state['test_grid'][i][n], \\\n",
    "                clf, x_transformer, y_transformer, top_t, i, n, test = True, \\\n",
    "                ff = state['grid'].getFeaturesFolder(state['feature_submission_name'])))\n",
    "        if len(state['cv_grid'][i][n]) > 0:\n",
    "            cv_preds.append(predict_single_grid_cell(state['cv_grid'][i][n], \\\n",
    "                clf, x_transformer, y_transformer, top_t, i, n))\n",
    "        del(clf)\n",
    "\n",
    "    if len(test_preds) > 0:\n",
    "        test_row = np.vstack(test_preds)\n",
    "    else:\n",
    "        test_row = None\n",
    "    if len(cv_preds) > 0:\n",
    "        cv_row = np.vstack(cv_preds)\n",
    "    else:\n",
    "        cv_row = None\n",
    "    print \"time taken for row %s is %s\" %(i, time.time() - init_row_time)\n",
    "    return (test_row, cv_row)\n",
    "\n",
    "def train_single_grid_cell(m, n, state):\n",
    "    # print m, n\n",
    "    t = 10\n",
    "    folder = state['folder']\n",
    "    data = np.loadtxt(state['grid'].getGridFile(m, n), dtype = float, delimiter = ',')\n",
    "    top_t = sorted(state['grid'].M[m][n].items(), cmp = lambda x, y: cmp(x[1], y[1]), reverse = True)[:t]\n",
    "    top_t = map(lambda x: x[0], top_t)\n",
    "    y = len(top_t)\n",
    "    if y < t:\n",
    "        top_t += [123]*(t-y) #[5348440074]*(t-y)\n",
    "    if len(data) == 0:\n",
    "        return None, None, None, top_t\n",
    "    if len(data.shape) == 1:\n",
    "        return None, None, None, top_t\n",
    "    mask = np.array(map(lambda x: state['grid'].M[m][n][x] > state['threshold'], data[:, 5]))\n",
    "    masked_data = data[mask, :]\n",
    "    if len(masked_data) < 10:\n",
    "        return None, None, None, top_t\n",
    "    X, x_transformer = trans_x(masked_data[:, (1, 2, 3, 4)])\n",
    "    Y, y_transformer = trans_y(masked_data[:, 5])\n",
    "\n",
    "    if len(Y) == 0:\n",
    "        return None, None, None, top_t, top_t_train_preds\n",
    "    else:\n",
    "        params = dict(state['params_dict'][m][n])\n",
    "        params['num_class'] = len(y_transformer['encoder'].classes_)\n",
    "        bst = classifier(X, Y, params)\n",
    "        return bst, x_transformer, y_transformer, top_t\n",
    "    pass\n",
    "\n",
    "def predict_single_grid_cell(X, clf, x_transformer, y_transformer, top_t):\n",
    "    t = 10\n",
    "    data = np.array(X)\n",
    "    if clf == None:\n",
    "        top_t_placeids = np.array([top_t]*len(data))\n",
    "    else:\n",
    "        temp_x = trans_x(data[:, (1, 2, 3, 4)], x_transformer)[0]\n",
    "        dtest = xgb.DMatrix(temp_x)\n",
    "        prediction_probs = clf.predict(dtest)\n",
    "        if len(prediction_probs.shape) == 1:\n",
    "            prediction_probs = prediction_probs.reshape(-1, 1)\n",
    "        top_t_placeids = y_transformer['encoder'].inverse_transform(np.argsort(prediction_probs, axis = 1)[:, ::-1][:, :t])\n",
    "        x, y = top_t_placeids.shape\n",
    "        if y < t:\n",
    "            temp_array = np.array([top_t[:(t-y)]]*len(top_t_placeids))\n",
    "            top_t_placeids = np.hstack((top_t_placeids, temp_array))\n",
    "    return np.hstack((data[:, 0].reshape(-1, 1), top_t_placeids))\n",
    "\n",
    "\n",
    "def trans_x(X, x_transformer = None):\n",
    "    \"\"\"\n",
    "    X = [[x, y, a, t]] nx4 array and each row contais x, y, a, t\n",
    "    x_transformer object contains additional information that are used to transform test data\n",
    "    when its None, you should generate it and return it along with the scaled data. In this case transforming\n",
    "    test data doenst need any extra. It will be useful when you use something like StandardSclaler, you need the\n",
    "    encoder to transform test data\n",
    "    \"\"\"\n",
    "    minute_v = X[:, 3]%60\n",
    "    hour_v = X[:, 3]//60\n",
    "    weekday_v = hour_v//24\n",
    "    month_v = weekday_v//30\n",
    "    year_v = (weekday_v//365 + 1)\n",
    "    hour_v = ((hour_v%24 + 1) + minute_v/60.0)\n",
    "    hour_v_2 = (X[:, 3]%(60*60*24))//(60*60*2)\n",
    "    hour_v_3 = (X[:, 3]%(60*60*24))//(60*60*3)\n",
    "    hour_v_4 = (X[:, 3]%(60*60*24))//(60*60*4)\n",
    "    hour_v_6 = (X[:, 3]%(60*60*24))//(60*60*6)\n",
    "    hour_v_8 = (X[:, 3]%(60*60*24))//(60*60*8)\n",
    "    weekday_v = (weekday_v%7 + 1)\n",
    "    month_v = (month_v%12 +1)\n",
    "    accuracy_v = np.log10(X[:, 2])\n",
    "\n",
    "    x_v = X[:, 0]\n",
    "    y_v = X[:, 1]\n",
    "    X_new = np.hstack((x_v.reshape(-1, 1),\\\n",
    "                     y_v.reshape(-1, 1),\\\n",
    "                     accuracy_v.reshape(-1, 1),\\\n",
    "                     hour_v.reshape(-1, 1),\\\n",
    "                     hour_v_2.reshape(-1, 1),\\\n",
    "                     hour_v_3.reshape(-1, 1),\\\n",
    "                     hour_v_4.reshape(-1, 1),\\\n",
    "                     hour_v_6.reshape(-1, 1),\\\n",
    "                     hour_v_8.reshape(-1, 1),\\\n",
    "                     weekday_v.reshape(-1, 1),\\\n",
    "                     month_v.reshape(-1, 1),\\\n",
    "                     year_v.reshape(-1, 1)))\n",
    "    return (X_new, x_transformer)\n",
    "\n",
    "def trans_y(y, y_transformer = None):\n",
    "    \"\"\"\n",
    "    transform y to proper encoding and return y and its encoder\n",
    "    \"\"\"\n",
    "    y = y.astype(int)\n",
    "    if y_transformer == None:\n",
    "        label_encoder = LabelEncoder()\n",
    "        label_encoder.fit(y)\n",
    "        y_transformer = {'encoder': label_encoder}\n",
    "    new_y = y_transformer['encoder'].transform(y).reshape(-1, 1)\n",
    "    return (new_y, y_transformer)\n",
    "\n",
    "\n",
    "class XGB_Model(SklearnModel):\n",
    "\n",
    "    def ini_params(self, params = default_xgb_params):\n",
    "        self.params = params\n",
    "        \n",
    "    def train_and_predict_parallel(self, submission_file, upload_to_s3 = False):\n",
    "        logging.debug('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "        init_time = time.time()\n",
    "        \n",
    "        # loading cv and test data into files\n",
    "        cv_data = np.loadtxt(self.cross_validation_file, dtype = float, delimiter = ',')\n",
    "        test_data = np.loadtxt(self.test_file, dtype = float, delimiter = ',')\n",
    "        \n",
    "        \n",
    "        test_grid_wise_data = [[[] for n in range(self.grid.max_n + 1)]\\\n",
    "            for m in range(self.grid.max_m + 1)]\n",
    "        cv_grid_wise_data = [[[] for n in range(self.grid.max_n + 1)]\\\n",
    "            for m in range(self.grid.max_m + 1)]\n",
    "\n",
    "        print \"converting test data to grid wise\"\n",
    "        for i in range(len(test_data)):\n",
    "            m, n = get_grids_of_a_point((test_data[i][1], test_data[i][2]), self.grid)[0]\n",
    "            test_grid_wise_data[m][n].append(test_data[i])\n",
    "\n",
    "        print \"converting cv data to grid wise\"\n",
    "        for i in range(len(cv_data)):\n",
    "            m, n = get_grids_of_a_point((cv_data[i][1], cv_data[i][2]), self.grid)[0]\n",
    "            cv_grid_wise_data[m][n].append(cv_data[i])\n",
    "\n",
    "        state = {}\n",
    "        state['grid'] = self.grid\n",
    "        state['cv_grid'] = cv_grid_wise_data\n",
    "        state['test_grid'] = test_grid_wise_data\n",
    "        state['threshold'] = self.threshold\n",
    "\n",
    "        xgb_params = self.params\n",
    "\n",
    "        paramsFile = None # '../correct_row_wise_grid_search_results.pickle'# self.grid.getParamsFile(5, 123340)\n",
    "        if paramsFile == None:\n",
    "            print \"params file doesn't exist.. so loading default params\"\n",
    "            state['params_dict'] = [[xgb_params for n in range(self.grid.max_n + 1)]\\\n",
    "                                        for m in range(self.grid.max_m + 1)]\n",
    "        else:\n",
    "            state['params_dict'] = pickle.load(open(paramsFile, 'rb'))\n",
    "\n",
    "        submission_name = os.path.basename(submission_file)[:-4]\n",
    "\n",
    "\n",
    "        p = Pool(8)\n",
    "        row_results = p.map(StateLoader(state), range(self.grid.max_m + 1))\n",
    "        p.close()\n",
    "        p.join()\n",
    "        del(test_grid_wise_data)\n",
    "        del(cv_grid_wise_data)\n",
    "        print \"Training time of parallel processing %s\" %(time.time() - init_time)\n",
    "        logging.debug(\"Training time of parallel processing %s\" %(time.time() - init_time))\n",
    "        \n",
    "        test_rows = map(lambda x: x[0], row_results)\n",
    "        test_rows = filter(lambda x: x != None, test_rows)\n",
    "        test_preds = np.vstack(test_rows).astype(int)\n",
    "        sorted_test = test_preds[test_preds[:, 0].argsort()]\n",
    "        \n",
    "        cv_rows = filter(lambda x: x != None, cv_rows)\n",
    "        cv_rows = map(lambda x: x[1], row_results)\n",
    "        cv_preds = np.vstack(cv_rows).astype(int)\n",
    "        sorted_cv = cv_preds[cv_preds[:, 0].argsort()]\n",
    "        actual_cv = cv_data[:, -1].astype(int).reshape(-1, 1)\n",
    "        cv_a_p = np.hstack((sorted_cv, actual_cv))\n",
    "        \n",
    "        apk_list = map(lambda row: apk(row[-1:], row[1:4]), cv_a_p)\n",
    "        self.cv_mean_precision = np.mean(apk_list)\n",
    "        print \"mean precision of cross validation set\", str(self.cv_mean_precision)\n",
    "        logging.debug(\"mean precision of cross validation set: \" + str(self.cv_mean_precision))\n",
    "        logging.debug(submission_file)\n",
    "        logging.debug(\"@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\")\n",
    "        \n",
    "        sorted_test = sorted_test.astype(str)\n",
    "        submission = open(submission_file, 'wb')\n",
    "        submission.write('row_id,place_id\\n')\n",
    "        for i in range(len(sorted_test)):\n",
    "            row = sorted_test[i]\n",
    "            row_id = row[0]\n",
    "            row_prediction_string = ' '.join(row[1:4])\n",
    "            submission.write(row_id + ',' + row_prediction_string + '\\n')\n",
    "            if i % 1000000 == 0:\n",
    "                print \"generating %s row of test data\" %(i)\n",
    "        submission.close()\n",
    "        if upload_to_s3:\n",
    "            zip_file_and_upload_to_s3(submission_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# param search for xgboost\n",
    "# I also had to impliment grid search for xgboost.train, cause I don't think sklearn wrapper is really complete\n",
    "\n",
    "# all you have to do is run\n",
    "# > python param_search.py 250 50 20 5 21 196 7\n",
    "# the above arguments looks for the best params in the grid defn 250, 50, 20, 5.. and the next two args tells what\n",
    "# grid cell to do the param search on, and the last argument defines the threshold, which eliminates place ids with \n",
    "# lesser threshold than that.\n",
    "\n",
    "# it also runs 8 jobs in parallel, along with each xgboost with 4 threads\n",
    "\n",
    "# if the files related to that above grid defn doesnt exist, it will generate and then does the performs the params search\n",
    "\n",
    "g = grid.Grid(250, 50, 20, 5, pref = 'grid', files_flag = True)\n",
    "g.generateCardinalityMatrix()\n",
    "\n",
    "m = 12 # row no\n",
    "n = 40 # column no\n",
    "th = 7 # threshold\n",
    "\n",
    "def transform_x(X, x_transformer = None):\n",
    "    \"\"\"\n",
    "    X = [[x, y, a, t]]\n",
    "    \"\"\"\n",
    "    minute_v = X[:, 3]%60\n",
    "    hour_v = X[:, 3]//60\n",
    "    weekday_v = hour_v//24\n",
    "    month_v = weekday_v//30\n",
    "    year_v = (weekday_v//365 + 1)\n",
    "    hour_v = ((hour_v%24 + 1) + minute_v/60.0)\n",
    "    hour_v_2 = (X[:, 3]%(60*60*24))//(60*60*2)\n",
    "    hour_v_3 = (X[:, 3]%(60*60*24))//(60*60*3)\n",
    "    hour_v_4 = (X[:, 3]%(60*60*24))//(60*60*4)\n",
    "    hour_v_6 = (X[:, 3]%(60*60*24))//(60*60*6)\n",
    "    hour_v_8 = (X[:, 3]%(60*60*24))//(60*60*8)\n",
    "    weekday_v = (weekday_v%7 + 1)\n",
    "    month_v = (month_v%12 +1)\n",
    "    accuracy_v = np.log10(X[:, 2])\n",
    "    x_v = X[:, 0]\n",
    "    y_v = X[:, 1]\n",
    "    return (np.hstack((x_v.reshape(-1, 1),\n",
    "                     y_v.reshape(-1, 1),\n",
    "                     accuracy_v.reshape(-1, 1),\n",
    "                     hour_v.reshape(-1, 1),\n",
    "                     hour_v_2.reshape(-1, 1),\n",
    "                     hour_v_3.reshape(-1, 1),\n",
    "                     hour_v_4.reshape(-1, 1),\n",
    "                     hour_v_6.reshape(-1, 1),\n",
    "                     hour_v_8.reshape(-1, 1),\n",
    "                     weekday_v.reshape(-1, 1),\n",
    "                     month_v.reshape(-1, 1),\n",
    "                     year_v.reshape(-1, 1)), x_transformer)\n",
    "\n",
    "def transform_y(y, y_transformer = None):\n",
    "    \"\"\"\n",
    "    place_ids to encoded array\n",
    "    \"\"\"\n",
    "    y = y.astype(int)\n",
    "    if y_transformer == None:\n",
    "        label_encoder = LabelEncoder()\n",
    "        label_encoder.fit(y)\n",
    "        y_transformer = {'encoder': label_encoder}\n",
    "    new_y = y_transformer['encoder'].transform(y).reshape(-1, 1)\n",
    "    return (new_y, y_transformer)\n",
    "\n",
    "def map3eval(preds, dtrain):\n",
    "    actual = dtrain.get_label()\n",
    "    predicted = preds.argsort(axis=1)[:,-np.arange(1,4)]\n",
    "    metric = 0.\n",
    "    for i in range(3):\n",
    "        metric += np.sum(actual==predicted[:,i])/(i+1)\n",
    "    metric /= actual.shape[0]\n",
    "    return 'MAP@3', metric\n",
    "\n",
    "def load_data(m, n):\n",
    "    f = g.getGridFile(m, n)\n",
    "    return np.loadtxt(f, delimiter = ',')\n",
    "\n",
    "def get_preds(probs, encoder):\n",
    "    return encoder.inverse_transform(np.argsort(probs, axis = 1)[:, ::-1][:, :3])\n",
    "\n",
    "def get_dtrain_enc(m, n):\n",
    "    data = load_data(m, n)\n",
    "    M = g.M\n",
    "    mask = np.array(map(lambda x: M[m][n][x] > th, data[:, 5]))\n",
    "    train = data[mask, :]\n",
    "    print data.shape, \"data_shape\"\n",
    "    X, x_tr = transform_x(train[:, (1, 2, 3, 4)])\n",
    "    y, enc = transform_y(train[:, 5])\n",
    "    print X.shape, \"X shape\"\n",
    "    print y.shape, \"y shape\"\n",
    "    print len(enc['encoder'].classes_), \"no of classes\"\n",
    "    logging.debug(\"no of classes: %s\" %(len(enc['encoder'].classes_)))\n",
    "\n",
    "    dtrain = xgb.DMatrix(X, label=np.ravel(y))\n",
    "    return (dtrain, enc)\n",
    "\n",
    "\n",
    "tup = lambda t: list(itertools.izip(itertools.repeat(t[0]), t[1]))\n",
    "\n",
    "def get_list_of_params(params_range):\n",
    "    pr = list(map(tup, params_range.items()))\n",
    "    pro = map(dict, list(itertools.product(*pr)))\n",
    "    return pro\n",
    "\n",
    "def grid_search_xgb(params_range_dict):\n",
    "    grid_params_list = get_list_of_params(params_range_dict)\n",
    "    p = Pool(8)\n",
    "    maps = p.map(get_map_of_xgb, grid_params_list)\n",
    "    p.close()\n",
    "    p.join()\n",
    "    sorted_maps = sorted(maps, cmp = lambda x, y: cmp(x['map'], y['map']), reverse = True)\n",
    "    logging.debug(\"top map results\")\n",
    "    logging.debug(sorted_maps[:3])\n",
    "    return sorted_maps\n",
    "\n",
    "def get_map_of_xgb(grid_param):\n",
    "    cv_params = dict(orig_params)\n",
    "    num_class = {'num_class': len(enc['encoder'].classes_)}\n",
    "    cv_params.update(num_class)\n",
    "    cv_params.update(grid_param)\n",
    "    # print orig_params, grid_param\n",
    "    temp_cv = xgb.cv(cv_params, dtrain, num_boost_round = 100,\n",
    "             early_stopping_rounds = 20, feval = map3eval, maximize = True)\n",
    "    temp_map = temp_cv['test-MAP@3-mean'][temp_cv.shape[0]-1]\n",
    "    grid_param['map'] = temp_map\n",
    "    # print \"cv results\", grid_param\n",
    "    return grid_param\n",
    "\n",
    "#m, n = (12, 50) (37, 50) (12, 150) (37, 150)\n",
    "#dtrain, enc = get_dtrain_enc(12, 50)\n",
    "param_range1 = {\n",
    "    'max_depth': range(2, 10, 1),\n",
    "    'min_child_weight': range(1, 7, 1)\n",
    "}\n",
    "\n",
    "param_range2 = {\n",
    "    'gamma': [i/10.0 for i in range(0, 6)],\n",
    "    'colsample_bylevel': [i/10.0 for i in range(5, 10)]\n",
    "}\n",
    "\n",
    "param_range3 = {\n",
    "    'max_delta_step': range(3, 15)\n",
    "}\n",
    "\n",
    "param_range4 = {\n",
    "    'subsample': [i/100.0 for i in range(60, 100, 5)],\n",
    "    'colsample_bytree': [i/100.0 for i in range(60, 100, 5)]\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "orig_params = {\n",
    "    'nthread': 4,\n",
    "    'eta': 0.1,\n",
    "    'objective': 'multi:softprob',\n",
    "    'max_depth': 8,\n",
    "    'min_child_weight': 5,\n",
    "    'gamma': 0.32,\n",
    "    'subsample': 0.9,\n",
    "    'colsample_bytree': 0.7,\n",
    "    'scale_pos_weight': 1,\n",
    "    'silent': 1\n",
    "}\n",
    "logging.debug(\"@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\")\n",
    "dtrain, enc = get_dtrain_enc(m, n)\n",
    "for param_range in [param_range1, param_range2, param_range3,\\\n",
    "    param_range4]:\n",
    "    result = None\n",
    "    try:\n",
    "        result = grid_search_xgb(param_range)\n",
    "    except Exception, e:\n",
    "        print e\n",
    "        print traceback.format_exc()\n",
    "    if result != None:\n",
    "        temp_param = result[0]\n",
    "        del(temp_param['map'])\n",
    "        orig_params.update(temp_param)\n",
    "        logging.debug(\"best params so far\")\n",
    "        logging.debug(orig_params)\n",
    "\n",
    "logging.debug(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
    "logging.debug(\"best params for the grid %s, %s, %s, %s\" %(g.X, g.Y, g.xd, g.yd))\n",
    "logging.debug(\"grid no m, n %s, %s\" %(m, n))\n",
    "logging.debug(\"threshold: %s\" %(th))\n",
    "logging.debug(orig_params)\n",
    "logging.debug(\"###################################################\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# miscellaneous functions\n",
    "\n",
    "from launch_spot_instance import get_spot_prices, request_spot\n",
    "# for this to work, you have to update sg, subnet, ami, key_name variables in the file, along with\n",
    "# aws_config.py with your config keys\n",
    "\n",
    "get_spot_prices(instance_type = 'c4.4xlarge')\n",
    "# this will print the past hour spot price of the instance in sorted order in av zones\n",
    "request_spot(price = '0.9', instance_type = 'm4.large', av_zone = 'us-east-1b', name = 'launched from notebook')\n",
    "# this will request a spot instance.. and waits till the machine launches and prints the machines public ip\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from helpers import zip_file_and_upload_to_s3\n",
    "\n",
    "zip_file_and_upload_to_s3(file_name, bucket_name)\n",
    "# zips the given file and uploads it to s3, aws_config should be updated for this to work"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
